{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from absl import logging\n",
    "from typing import Text\n",
    "from tfx.orchestration import pipeline, metadata\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define\n",
    "\n",
    "Pertama, akan mengatur konfigurasi pipeline TFX dengan menentukan nama pipeline, lokasi data, path modul yang diperlukan, serta direktori output untuk menyimpan hasil proses pipeline machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_NAME: mellisadmyn-pipeline\n",
      "DATA_ROOT: data\n",
      "TRANSFORM_MODULE_FILE: modules\\transform.py\n",
      "TRAINER_MODULE_FILE: modules\\trainer.py\n",
      "TUNER_MODULE_FILE: modules\\tuner.py\n",
      "COMPONENTS_MODULE_FILE: modules\\components.py\n",
      "OUTPUT_BASE: output\n",
      "serving_model_dir: output\\serving_model\n",
      "pipeline_root: output\\mellisadmyn-pipeline\n",
      "metadata_path: output\\mellisadmyn-pipeline\\metadata.sqlite\n"
     ]
    }
   ],
   "source": [
    "def get_pipeline_variables():\n",
    "    # Nama pipeline\n",
    "    PIPELINE_NAME = \"mellisadmyn-pipeline\"\n",
    "\n",
    "    # Pipeline inputs\n",
    "    DATA_ROOT = \"data\"\n",
    "    MODULES_FOLDER = \"modules\"\n",
    "    TRANSFORM_MODULE_FILE = os.path.join(MODULES_FOLDER, \"transform.py\")\n",
    "    TRAINER_MODULE_FILE = os.path.join(MODULES_FOLDER, \"trainer.py\")\n",
    "    TUNER_MODULE_FILE = os.path.join(MODULES_FOLDER, \"tuner.py\")\n",
    "    COMPONENTS_MODULE_FILE = os.path.join(MODULES_FOLDER, \"components.py\")\n",
    "\n",
    "    # Pipeline outputs\n",
    "    OUTPUT_BASE = \"output\"\n",
    "    serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
    "    pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
    "    metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")\n",
    "\n",
    "    # Buat folder modules jika belum ada\n",
    "    os.makedirs(MODULES_FOLDER, exist_ok=True)\n",
    "\n",
    "    # Buat file kosong jika belum ada\n",
    "    for file in [TRANSFORM_MODULE_FILE, TRAINER_MODULE_FILE, TUNER_MODULE_FILE, COMPONENTS_MODULE_FILE]:\n",
    "        if not os.path.exists(file):\n",
    "            with open(file, 'w') as f:\n",
    "                f.write(f\"# {os.path.basename(file)} file for pipeline.\\n\")\n",
    "\n",
    "    # Return all variables as separate values\n",
    "    return (\n",
    "        PIPELINE_NAME,\n",
    "        DATA_ROOT,\n",
    "        TRANSFORM_MODULE_FILE,\n",
    "        TRAINER_MODULE_FILE,\n",
    "        TUNER_MODULE_FILE,\n",
    "        COMPONENTS_MODULE_FILE,\n",
    "        OUTPUT_BASE,\n",
    "        serving_model_dir,\n",
    "        pipeline_root,\n",
    "        metadata_path,\n",
    "    )\n",
    "\n",
    "# Memanggil fungsi dan meng-unpack variabel\n",
    "PIPELINE_NAME, DATA_ROOT, TRANSFORM_MODULE_FILE, TRAINER_MODULE_FILE, TUNER_MODULE_FILE, COMPONENTS_MODULE_FILE, OUTPUT_BASE, serving_model_dir, pipeline_root, metadata_path = get_pipeline_variables()\n",
    "\n",
    "# Print variables\n",
    "print(\"PIPELINE_NAME:\", PIPELINE_NAME)\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"TRANSFORM_MODULE_FILE:\", TRANSFORM_MODULE_FILE)\n",
    "print(\"TRAINER_MODULE_FILE:\", TRAINER_MODULE_FILE)\n",
    "print(\"TUNER_MODULE_FILE:\", TUNER_MODULE_FILE)\n",
    "print(\"COMPONENTS_MODULE_FILE:\", COMPONENTS_MODULE_FILE)\n",
    "print(\"OUTPUT_BASE:\", OUTPUT_BASE)\n",
    "print(\"serving_model_dir:\", serving_model_dir)\n",
    "print(\"pipeline_root:\", pipeline_root)\n",
    "print(\"metadata_path:\", metadata_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lalu, menginisialisasi komponen-komponen TFX yang membangun pipeline machine learning end-to-end, mulai dari membaca dan memproses data, melakukan validasi dan transformasi, melakukan tuning dan pelatihan model, mengevaluasi kinerja model, hingga menyimpan model yang lolos evaluasi untuk keperluan deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama adalah membuat components.py\n",
    "\n",
    "Kode berikut mendefinisikan fungsi `init_components`, yang menginisialisasi semua komponen TFX seperti pembacaan data (`CsvExampleGen`), validasi data (`ExampleValidator`), transformasi fitur (`Transform`), tuning hyperparameter (`Tuner`), pelatihan model (`Trainer`), evaluasi model (`Evaluator`), hingga penyimpanan model yang telah dilatih dan lolos evaluasi (`Pusher`), untuk membangun pipeline machine learning end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {COMPONENTS_MODULE_FILE}\n",
    "\"\"\"\n",
    "Modul ini berisi fungsi untuk menginisialisasi semua komponen TFX yang\n",
    "digunakan dalam pipeline machine learning end-to-end.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import (\n",
    "    CsvExampleGen,\n",
    "    StatisticsGen,\n",
    "    SchemaGen,\n",
    "    ExampleValidator,\n",
    "    Transform,\n",
    "    Trainer,\n",
    "    Tuner,\n",
    "    Evaluator,\n",
    "    Pusher,\n",
    ")\n",
    "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import (\n",
    "    LatestBlessedModelStrategy,\n",
    ")\n",
    "\n",
    "# Fungsi untuk melakukan inisialisasi komponen\n",
    "def init_components(config):\n",
    "    \"\"\"\n",
    "    Mengembalikan komponen TFX untuk pipeline.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Dictionary konfigurasi dengan path dan pengaturan.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Komponen pipeline TFX.\n",
    "    \"\"\"\n",
    "\n",
    "    # Membagi dataset dengan perbandingan 80% untuk training dan 20% untuk evaluasi\n",
    "    output = example_gen_pb2.Output(    # pylint: disable=no-member\n",
    "        split_config=example_gen_pb2.SplitConfig(   # pylint: disable=no-member\n",
    "            splits=[\n",
    "                example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),    # pylint: disable=no-member\n",
    "                example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2),     # pylint: disable=no-member\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Komponen example gen\n",
    "    example_gen = CsvExampleGen(\n",
    "        input_base=config[\"DATA_ROOT\"],\n",
    "        output_config=output,\n",
    "    )\n",
    "\n",
    "    # Komponen statistics gen\n",
    "    statistics_gen = StatisticsGen(\n",
    "        examples=example_gen.outputs[\"examples\"]\n",
    "    )\n",
    "\n",
    "    # Komponen schema gen\n",
    "    schema_gen = SchemaGen(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"]\n",
    "    )\n",
    "\n",
    "    # Komponen example validator\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "    )\n",
    "\n",
    "    # Komponen transform menggunakan modul transform.py\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        module_file=os.path.abspath(config[\"transform_module\"]),\n",
    "    )\n",
    "\n",
    "    # Komponen tuner menggunakan modul tuner.py\n",
    "    tuner = Tuner(\n",
    "        module_file=os.path.abspath(config[\"tuner_module\"]),\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        train_args=trainer_pb2.TrainArgs(  # pylint: disable=no-member\n",
    "            splits=[\"train\"],\n",
    "            num_steps=config[\"training_steps\"],\n",
    "        ),\n",
    "        eval_args=trainer_pb2.EvalArgs(  # pylint: disable=no-member\n",
    "            splits=[\"eval\"],\n",
    "            num_steps=config[\"eval_steps\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Komponen trainer menggunakan modul trainer.py\n",
    "    trainer = Trainer(\n",
    "        module_file=os.path.abspath(config[\"training_module\"]),\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        hyperparameters=tuner.outputs[\"best_hyperparameters\"],\n",
    "        train_args=trainer_pb2.TrainArgs(  # pylint: disable=no-member\n",
    "            splits=[\"train\"],\n",
    "            num_steps=config[\"training_steps\"],\n",
    "        ),\n",
    "        eval_args=trainer_pb2.EvalArgs(  # pylint: disable=no-member\n",
    "            splits=[\"eval\"],\n",
    "            num_steps=config[\"eval_steps\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Komponen model resolver\n",
    "    model_resolver = Resolver(\n",
    "        strategy_class=LatestBlessedModelStrategy,\n",
    "        model=Channel(type=Model),\n",
    "        model_blessing=Channel(type=ModelBlessing),\n",
    "    ).with_id(\"Latest_blessed_model_resolver\")\n",
    "\n",
    "    # Konfigurasi metrik evaluasi\n",
    "    metrics_specs = [\n",
    "        tfma.MetricsSpec(\n",
    "            metrics=[\n",
    "                tfma.MetricConfig(class_name=\"AUC\"),\n",
    "                tfma.MetricConfig(class_name=\"Precision\"),\n",
    "                tfma.MetricConfig(class_name=\"Recall\"),\n",
    "                tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
    "                tfma.MetricConfig(\n",
    "                    class_name=\"BinaryAccuracy\",\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={\"value\": 0.8}\n",
    "                        ),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={\"value\": 0.0001},\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Konfigurasi evaluasi\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key=\"Attrition\")],\n",
    "        slicing_specs=[tfma.SlicingSpec()],\n",
    "        metrics_specs=metrics_specs,\n",
    "    )\n",
    "\n",
    "    # Komponen evaluator\n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        baseline_model=model_resolver.outputs[\"model\"],\n",
    "        eval_config=eval_config,\n",
    "    )\n",
    "\n",
    "    # Komponen pusher\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        model_blessing=evaluator.outputs[\"blessing\"],\n",
    "        push_destination=pusher_pb2.PushDestination(  # pylint: disable=no-member\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(  # pylint: disable=no-member\n",
    "                base_directory=config[\"serving_model_dir\"]\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Mengembalikan semua komponen pipeline\n",
    "    return (\n",
    "        example_gen,\n",
    "        statistics_gen,\n",
    "        schema_gen,\n",
    "        example_validator,\n",
    "        transform,\n",
    "        tuner,\n",
    "        trainer,\n",
    "        model_resolver,\n",
    "        evaluator,\n",
    "        pusher,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lalu, yang kedua adalah mendefinisikan file `transform.py`,\n",
    "\n",
    "yang berisi fungsi `preprocessing_fn` untuk melakukan preprocessing data. Preprocessing ini mencakup normalisasi fitur numerik (`TotalWorkingYears`, `Age`, `MonthlyIncome`), encoding fitur kategorikal (`OverTime`, `MaritalStatus`, `JobRole`, `Department`), dan transformasi label (`Attrition`) menjadi nilai numerik, sehingga data siap digunakan oleh komponen pipeline lainnya seperti pelatihan model (`Trainer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "\"\"\"\n",
    "Modul ini berisi fungsi untuk melakukan preprocessing dataset menggunakan TFX Transform.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# Daftar numerical fitur pada dataset\n",
    "NUMERICAL_FEATURES = [\n",
    "    \"TotalWorkingYears\",\n",
    "    \"Age\",\n",
    "    \"MonthlyIncome\",\n",
    "]\n",
    "\n",
    "# Daftar categorical fitur pada dataset\n",
    "CATEGORICAL_FEATURES = {\n",
    "    \"OverTime\": 2,  # Yes/No -> Binary (2 unique values)\n",
    "    \"MaritalStatus\": 3,  # Married/Single/Divorced -> 3 unique values\n",
    "    \"JobRole\": 9,  # Healthcare Representative, Sales Executive, etc. -> 9 unique values\n",
    "    \"Department\": 3,  # Research & Development, Sales, etc. -> 3 unique values\n",
    "}\n",
    "\n",
    "# Label key\n",
    "LABEL_KEY = \"Attrition\"\n",
    "\n",
    "# Fungsi untuk mengubah nama fitur yang sudah ditransformasi\n",
    "def transformed_name(key):\n",
    "    \"\"\"Renaming transformed features.\"\"\"\n",
    "    return key + \"_xf\"\n",
    "\n",
    "# Fungsi untuk melakukan preprocessing\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"\n",
    "    Preprocess input features into transformed features.\n",
    "\n",
    "    Args:\n",
    "        inputs: Map dari kunci fitur ke raw features.\n",
    "\n",
    "    Returns:\n",
    "        outputs: Map dari kunci fitur ke fitur yang telah ditransformasi.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "\n",
    "    # Transformasi untuk fitur numerik\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        outputs[transformed_name(feature)] = tft.scale_to_0_1(inputs[feature])\n",
    "\n",
    "    # Transformasi untuk fitur kategorikal\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        outputs[transformed_name(feature)] = tft.compute_and_apply_vocabulary(\n",
    "            inputs[feature]\n",
    "        )\n",
    "\n",
    "    # Transformasi untuk label\n",
    "    outputs[transformed_name(LABEL_KEY)] = tf.cast(\n",
    "        tft.compute_and_apply_vocabulary(inputs[LABEL_KEY]), tf.int64\n",
    "    )\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode ketiga yaitu mendefinisikan file `tuner.py`, \n",
    "\n",
    "yang berisi fungsi `model_builder` untuk membuat model deep learning dengan arsitektur fleksibel menggunakan hyperparameter tuning. Fungsi ini memungkinkan eksplorasi konfigurasi terbaik, seperti jumlah unit di hidden layer, dropout rate, dan learning rate, untuk tugas klasifikasi biner. \n",
    "\n",
    "Selain itu, fungsi `tuner_fn` digunakan untuk menginisialisasi proses tuning menggunakan `KerasTuner`, memanfaatkan dataset yang telah ditransformasikan, dan mencari kombinasi hyperparameter terbaik berdasarkan metrik validasi (`val_binary_accuracy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TUNER_MODULE_FILE}\n",
    "\"\"\"\n",
    "Modul ini mengatur fungsi untuk tuning hyperparameter\n",
    "model klasifikasi biner menggunakan Keras Tuner.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import tensorflow_transform as tft\n",
    "from tfx.v1.components import TunerFnResult\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "from transform import NUMERICAL_FEATURES, CATEGORICAL_FEATURES, transformed_name\n",
    "from trainer import input_fn\n",
    "\n",
    "# Fungsi untuk membuat input features (menghindari duplikasi kode)\n",
    "def create_input_features():\n",
    "    \"\"\"\n",
    "    Membuat input features untuk numerical dan categorical features.\n",
    "    \n",
    "    Returns:\n",
    "        list: Daftar input layers.\n",
    "    \"\"\"\n",
    "    input_features = []\n",
    "\n",
    "    # Input layers for numerical features\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "\n",
    "    # Input layers for categorical features\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "\n",
    "    return input_features\n",
    "\n",
    "# Fungsi untuk membuat model\n",
    "def model_builder(hyperparameters):\n",
    "    \"\"\"\n",
    "    Defines and returns a Keras model for binary classification.\n",
    "    \"\"\"\n",
    "    # Membuat input features\n",
    "    input_features = create_input_features()\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenate = tf.keras.layers.concatenate(input_features)\n",
    "\n",
    "    # Hidden layers with hyperparameter tuning\n",
    "    deep = tf.keras.layers.Dense(\n",
    "        hyperparameters.Choice('units_layer1', [64, 128, 256]),\n",
    "        activation=\"relu\")(concatenate)\n",
    "    deep = tf.keras.layers.Dropout(\n",
    "        hyperparameters.Choice('dropout_layer1', [0.2, 0.3, 0.4]))(deep)\n",
    "\n",
    "    deep = tf.keras.layers.Dense(\n",
    "        hyperparameters.Choice('units_layer2', [32, 64, 128]),\n",
    "        activation=\"relu\")(deep)\n",
    "    deep = tf.keras.layers.Dropout(\n",
    "        hyperparameters.Choice('dropout_layer2', [0.2, 0.3, 0.4]))(deep)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(deep)\n",
    "\n",
    "    # Compile the model\n",
    "    model = tf.keras.models.Model(inputs=input_features, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hyperparameters.Choice('learning_rate', [0.001, 0.0001])\n",
    "        ),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Fungsi tuner\n",
    "def tuner_fn(fn_args: FnArgs):\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning function for the model.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    # Load training and evaluation datasets\n",
    "    train_dataset = input_fn(fn_args.train_files, tf_transform_output, batch_size=64)\n",
    "    eval_dataset = input_fn(fn_args.eval_files, tf_transform_output, batch_size=64)\n",
    "\n",
    "    # Create tuner\n",
    "    tuner = kt.RandomSearch(\n",
    "        model_builder,\n",
    "        objective='val_binary_accuracy',\n",
    "        max_trials=10,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name='attrition_tuning'\n",
    "    )\n",
    "\n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            \"x\": train_dataset,\n",
    "            \"validation_data\": eval_dataset,\n",
    "            \"steps_per_epoch\": fn_args.train_steps,\n",
    "            \"validation_steps\": fn_args.eval_steps,\n",
    "            \"epochs\": 10\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, Kode keempat berikut mendefinisikan file `trainer.py`, \n",
    "\n",
    "yang berisi fungsi `get_model` untuk membangun arsitektur model deep learning menggunakan TensorFlow untuk tugas klasifikasi biner. Fungsi ini mencakup definisi input fitur (numerik dan kategorikal), hidden layer dengan aktivasi ReLU, dan layer output dengan aktivasi sigmoid. \n",
    "\n",
    "Selain itu, fungsi `run_fn` digunakan untuk melatih model menggunakan dataset yang telah diproses, dengan logging melalui TensorBoard, serta menyimpan model yang telah dilatih lengkap dengan serving signature untuk keperluan deployment. Visualisasi arsitektur model juga disimpan sebagai gambar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINER_MODULE_FILE}\n",
    "\"\"\"\n",
    "Modul ini berisi fungsi untuk pelatihan model machine learning\n",
    "menggunakan pipeline TFX, termasuk definisi model, proses pelatihan,\n",
    "dan evaluasi.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "\n",
    "from transform import (\n",
    "    NUMERICAL_FEATURES,\n",
    "    CATEGORICAL_FEATURES,\n",
    "    transformed_name,\n",
    "    LABEL_KEY,\n",
    ")\n",
    "\n",
    "# Fungsi untuk membuat model\n",
    "def get_model(show_summary=True):\n",
    "    \"\"\"\n",
    "    Defines a Keras model for binary classification.\n",
    "    \"\"\"\n",
    "    # Input layers for numerical features\n",
    "    input_features = []\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "\n",
    "    # Input layers for categorical features\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            tf.keras.Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenate = tf.keras.layers.concatenate(input_features)\n",
    "\n",
    "    # Hidden layers\n",
    "    deep = tf.keras.layers.Dense(128, activation=\"relu\")(concatenate)\n",
    "    deep = tf.keras.layers.Dropout(0.3)(deep)\n",
    "    deep = tf.keras.layers.Dense(64, activation=\"relu\")(deep)\n",
    "    deep = tf.keras.layers.Dropout(0.3)(deep)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(deep)\n",
    "\n",
    "    # Compile the model\n",
    "    model = tf.keras.models.Model(inputs=input_features, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "\n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Fungsi untuk membaca data yang sudah dikompresi\n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"Reads compressed data.\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    "\n",
    "# Fungsi untuk mendapatkan fitur yang sudah ditransformasi\n",
    "def get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"Returns a serving function for the model.\"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Parses a serialized tf.Example and returns model predictions.\"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "\n",
    "        outputs = model(transformed_features)\n",
    "        return {\"outputs\": outputs}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "# Fungsi untuk membuat dataset\n",
    "def input_fn(file_pattern, tf_transform_output, batch_size=64):\n",
    "    \"\"\"Generates features and labels for training/evaluation.\"\"\"\n",
    "    transformed_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy()\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        label_key=transformed_name(LABEL_KEY),\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Fungsi untuk menjalankan pelatihan model\n",
    "def run_fn(fn_args):\n",
    "    \"\"\"\n",
    "    Train the model using the given arguments.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    # Load training and evaluation datasets\n",
    "    train_dataset = input_fn(fn_args.train_files, tf_transform_output, batch_size=64)\n",
    "    eval_dataset = input_fn(fn_args.eval_files, tf_transform_output, batch_size=64)\n",
    "\n",
    "    # Define the model\n",
    "    model = get_model()\n",
    "\n",
    "    # Define callbacks\n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), \"logs\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, update_freq=\"batch\"\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=[tensorboard_callback],\n",
    "        epochs=10\n",
    "    )\n",
    "\n",
    "    # Save the model with serving signature\n",
    "    signatures = {\n",
    "        \"serving_default\": get_serve_tf_examples_fn(\n",
    "            model, tf_transform_output\n",
    "        ).get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n",
    "        ),\n",
    "    }\n",
    "    model.save(\n",
    "        fn_args.serving_model_dir, save_format=\"tf\", signatures=signatures\n",
    "    )\n",
    "\n",
    "    # Visualize the model architecture\n",
    "    plot_model(\n",
    "        model,\n",
    "        to_file='images/model_plot.png',\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah semua modules siap digunakan. Lanjut untuk mendefinisikan fungsi `init_local_pipeline` untuk menginisialisasi pipeline TFX lokal, dengan konfigurasi Apache Beam untuk eksekusi multi-processing, caching, dan koneksi metadata menggunakan SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_local_pipeline(\n",
    "    components, pipeline_root: str\n",
    ") -> pipeline.Pipeline:\n",
    "    \"\"\"\n",
    "    Initializes a local TFX pipeline.\n",
    "\n",
    "    Args:\n",
    "        components: List of TFX components to include in the pipeline.\n",
    "        pipeline_root: Path to the root directory for pipeline outputs.\n",
    "\n",
    "    Returns:\n",
    "        A TFX pipeline object.\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
    "\n",
    "    # Beam arguments for running the pipeline locally\n",
    "    beam_args = [\n",
    "        \"--direct_running_mode=multi_processing\",\n",
    "        \"--direct_num_workers=0\"  # Automatically detect number of workers\n",
    "    ]\n",
    "    \n",
    "    # Create the pipeline\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path\n",
    "        ),\n",
    "        beam_pipeline_args=beam_args\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lalu akan menjalankan pipeline TFX, dimulai dengan inisialisasi komponen pipeline berdasarkan konfigurasi yang telah ditentukan, kemudian membuat pipeline lokal menggunakan fungsi `init_local_pipeline`, dan akhirnya mengeksekusi pipeline menggunakan `BeamDagRunner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 34s]\n",
      "val_binary_accuracy: 0.8862500190734863\n",
      "\n",
      "Best val_binary_accuracy So Far: 0.8863750100135803\n",
      "Total elapsed time: 00h 06m 08s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:absl:Finished tuning... Tuner ID: tuner0\n",
      "INFO:absl:Best HyperParameters: {'space': [{'class_name': 'Choice', 'config': {'name': 'units_layer1', 'default': 64, 'conditions': [], 'values': [64, 128, 256], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'dropout_layer1', 'default': 0.2, 'conditions': [], 'values': [0.2, 0.3, 0.4], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'units_layer2', 'default': 32, 'conditions': [], 'values': [32, 64, 128], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'dropout_layer2', 'default': 0.2, 'conditions': [], 'values': [0.2, 0.3, 0.4], 'ordered': True}}, {'class_name': 'Choice', 'config': {'name': 'learning_rate', 'default': 0.001, 'conditions': [], 'values': [0.001, 0.0001], 'ordered': True}}], 'values': {'units_layer1': 64, 'dropout_layer1': 0.2, 'units_layer2': 64, 'dropout_layer2': 0.2, 'learning_rate': 0.0001}}\n",
      "INFO:absl:Best Hyperparameters are written to output\\mellisadmyn-pipeline\\Tuner\\best_hyperparameters\\7\\best_hyperparameters.txt.\n",
      "INFO:absl:Tuner results are written to output\\mellisadmyn-pipeline\\Tuner\\tuner_results\\7\\tuner_results.json.\n",
      "INFO:absl:Cleaning up stateless execution info.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Execution 7 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'tuner_results': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Tuner\\\\tuner_results\\\\7\"\n",
      ", artifact_type: name: \"TunerResults\"\n",
      ")], 'best_hyperparameters': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Tuner\\\\best_hyperparameters\\\\7\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}) for execution 7\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:node Tuner is finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in output\\mellisadmyn-pipeline\\Tuner\\.system\\executor_execution\\7\\.temp\\7\\attrition_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_binary_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 64\n",
      "dropout_layer1: 0.2\n",
      "units_layer2: 64\n",
      "dropout_layer2: 0.2\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8863750100135803\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 64\n",
      "dropout_layer1: 0.2\n",
      "units_layer2: 128\n",
      "dropout_layer2: 0.4\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8863124847412109\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 128\n",
      "dropout_layer1: 0.2\n",
      "units_layer2: 32\n",
      "dropout_layer2: 0.4\n",
      "learning_rate: 0.001\n",
      "Score: 0.8862500190734863\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 128\n",
      "dropout_layer1: 0.4\n",
      "units_layer2: 32\n",
      "dropout_layer2: 0.2\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8821250200271606\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 128\n",
      "dropout_layer1: 0.3\n",
      "units_layer2: 128\n",
      "dropout_layer2: 0.3\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8819375038146973\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 128\n",
      "dropout_layer1: 0.4\n",
      "units_layer2: 32\n",
      "dropout_layer2: 0.3\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8817499876022339\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 128\n",
      "dropout_layer1: 0.3\n",
      "units_layer2: 64\n",
      "dropout_layer2: 0.3\n",
      "learning_rate: 0.001\n",
      "Score: 0.8817499876022339\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 256\n",
      "dropout_layer1: 0.3\n",
      "units_layer2: 64\n",
      "dropout_layer2: 0.3\n",
      "learning_rate: 0.001\n",
      "Score: 0.8773750066757202\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 256\n",
      "dropout_layer1: 0.3\n",
      "units_layer2: 128\n",
      "dropout_layer2: 0.3\n",
      "learning_rate: 0.001\n",
      "Score: 0.8773124814033508\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units_layer1: 256\n",
      "dropout_layer1: 0.4\n",
      "units_layer2: 128\n",
      "dropout_layer2: 0.2\n",
      "learning_rate: 0.001\n",
      "Score: 0.8728125095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:node Trainer is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20241223-124048.244141\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transformed_examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Tuner\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Tuner\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"HyperParameters\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"best_hyperparameters\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"TransformGraph\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transform_graph\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 250,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"trainer@output\\\\mellisadmyn-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+e362364f639e83968f36543dbf88bd3ecee2a46ab5b26c0bbaed628252200e5a-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "upstream_nodes: \"Transform\"\n",
      "upstream_nodes: \"Tuner\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Trainer] Resolved inputs: ({'examples': [Artifact(artifact: id: 11\n",
      "type_id: 16\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Transform\\\\transformed_examples\\\\6\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"eval\\\", \\\"train\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929053437\n",
      "last_update_time_since_epoch: 1734929053437\n",
      ", artifact_type: id: 16\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'transform_graph': [Artifact(artifact: id: 7\n",
      "type_id: 24\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Transform\\\\transform_graph\\\\6\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929053436\n",
      "last_update_time_since_epoch: 1734929053436\n",
      ", artifact_type: id: 24\n",
      "name: \"TransformGraph\"\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734928944176\n",
      "last_update_time_since_epoch: 1734928944176\n",
      ", artifact_type: id: 20\n",
      "name: \"Schema\"\n",
      ")], 'hyperparameters': [Artifact(artifact: id: 14\n",
      "type_id: 28\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Tuner\\\\best_hyperparameters\\\\7\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929428081\n",
      "last_update_time_since_epoch: 1734929428081\n",
      ", artifact_type: id: 28\n",
      "name: \"HyperParameters\"\n",
      ")]},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 8\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'examples': [Artifact(artifact: id: 11\n",
      "type_id: 16\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Transform\\\\transformed_examples\\\\6\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"eval\\\", \\\"train\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929053437\n",
      "last_update_time_since_epoch: 1734929053437\n",
      ", artifact_type: id: 16\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'transform_graph': [Artifact(artifact: id: 7\n",
      "type_id: 24\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Transform\\\\transform_graph\\\\6\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929053436\n",
      "last_update_time_since_epoch: 1734929053436\n",
      ", artifact_type: id: 24\n",
      "name: \"TransformGraph\"\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734928944176\n",
      "last_update_time_since_epoch: 1734928944176\n",
      ", artifact_type: id: 20\n",
      "name: \"Schema\"\n",
      ")], 'hyperparameters': [Artifact(artifact: id: 14\n",
      "type_id: 28\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Tuner\\\\best_hyperparameters\\\\7\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929428081\n",
      "last_update_time_since_epoch: 1734929428081\n",
      ", artifact_type: id: 28\n",
      "name: \"HyperParameters\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_run': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Trainer\\\\model_run\\\\8\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")]}), exec_properties={'eval_args': '{\\n  \"num_steps\": 250,\\n  \"splits\": [\\n    \"eval\"\\n  ]\\n}', 'module_path': 'trainer@output\\\\mellisadmyn-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+e362364f639e83968f36543dbf88bd3ecee2a46ab5b26c0bbaed628252200e5a-py3-none-any.whl', 'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 1000,\\n  \"splits\": [\\n    \"train\"\\n  ]\\n}'}, execution_output_uri='output\\\\mellisadmyn-pipeline\\\\Trainer\\\\.system\\\\executor_execution\\\\8\\\\executor_output.pb', stateful_working_dir='output\\\\mellisadmyn-pipeline\\\\Trainer\\\\.system\\\\stateful_working_dir\\\\20241223-124048.244141', tmp_dir='output\\\\mellisadmyn-pipeline\\\\Trainer\\\\.system\\\\executor_execution\\\\8\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20241223-124048.244141\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transformed_examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Tuner\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Tuner\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"HyperParameters\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"best_hyperparameters\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"TransformGraph\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transform_graph\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 250,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"trainer@output\\\\mellisadmyn-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+e362364f639e83968f36543dbf88bd3ecee2a46ab5b26c0bbaed628252200e5a-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "upstream_nodes: \"Transform\"\n",
      "upstream_nodes: \"Tuner\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"mellisadmyn-pipeline\"\n",
      ", pipeline_run_id='20241223-124048.244141')\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "INFO:absl:udf_utils.get_fn {'eval_args': '{\\n  \"num_steps\": 250,\\n  \"splits\": [\\n    \"eval\"\\n  ]\\n}', 'module_path': 'trainer@output\\\\mellisadmyn-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+e362364f639e83968f36543dbf88bd3ecee2a46ab5b26c0bbaed628252200e5a-py3-none-any.whl', 'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 1000,\\n  \"splits\": [\\n    \"train\"\\n  ]\\n}'} 'run_fn'\n",
      "INFO:absl:Installing 'output\\\\mellisadmyn-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+e362364f639e83968f36543dbf88bd3ecee2a46ab5b26c0bbaed628252200e5a-py3-none-any.whl' to a temporary directory.\n",
      "INFO:absl:Executing: ['c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\python.exe', '-m', 'pip', 'install', '--target', 'C:\\\\Users\\\\melli\\\\AppData\\\\Local\\\\Temp\\\\tmpapwpg1ue', 'output\\\\mellisadmyn-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+e362364f639e83968f36543dbf88bd3ecee2a46ab5b26c0bbaed628252200e5a-py3-none-any.whl']\n",
      "INFO:absl:Successfully installed 'output\\\\mellisadmyn-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+e362364f639e83968f36543dbf88bd3ecee2a46ab5b26c0bbaed628252200e5a-py3-none-any.whl'.\n",
      "INFO:absl:Training model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " TotalWorkingYears_xf (InputLay  [(None, 1)]         0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " Age_xf (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " MonthlyIncome_xf (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " OverTime_xf (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " MaritalStatus_xf (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " JobRole_xf (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Department_xf (InputLayer)     [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 7)            0           ['TotalWorkingYears_xf[0][0]',   \n",
      "                                                                  'Age_xf[0][0]',                 \n",
      "                                                                  'MonthlyIncome_xf[0][0]',       \n",
      "                                                                  'OverTime_xf[0][0]',            \n",
      "                                                                  'MaritalStatus_xf[0][0]',       \n",
      "                                                                  'JobRole_xf[0][0]',             \n",
      "                                                                  'Department_xf[0][0]']          \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          1024        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            65          ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,345\n",
      "Trainable params: 9,345\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 5s 4ms/step - loss: 0.4141 - binary_accuracy: 0.8341 - val_loss: 0.3318 - val_binary_accuracy: 0.8821\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3746 - binary_accuracy: 0.8531 - val_loss: 0.3380 - val_binary_accuracy: 0.8771\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3571 - binary_accuracy: 0.8610 - val_loss: 0.3482 - val_binary_accuracy: 0.8727\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3432 - binary_accuracy: 0.8653 - val_loss: 0.3577 - val_binary_accuracy: 0.8635\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3300 - binary_accuracy: 0.8708 - val_loss: 0.3739 - val_binary_accuracy: 0.8636\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.3158 - binary_accuracy: 0.8764 - val_loss: 0.3898 - val_binary_accuracy: 0.8636\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3046 - binary_accuracy: 0.8805 - val_loss: 0.3997 - val_binary_accuracy: 0.8591\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.2942 - binary_accuracy: 0.8852 - val_loss: 0.4073 - val_binary_accuracy: 0.8549\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.2843 - binary_accuracy: 0.8884 - val_loss: 0.4194 - val_binary_accuracy: 0.8546\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.2749 - binary_accuracy: 0.8925 - val_loss: 0.4382 - val_binary_accuracy: 0.8363\n",
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output\\mellisadmyn-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output\\mellisadmyn-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Training complete. Model written to output\\mellisadmyn-pipeline\\Trainer\\model\\8\\Format-Serving. ModelRun written to output\\mellisadmyn-pipeline\\Trainer\\model_run\\8\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 8 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_run': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Trainer\\\\model_run\\\\8\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")]}) for execution 8\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:node Trainer is finished.\n",
      "INFO:absl:node Evaluator is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "    base_type: EVALUATE\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20241223-124048.244141\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Latest_blessed_model_resolver\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Latest_blessed_model_resolver\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"Attrition\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"fairness_indicator_thresholds\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Latest_blessed_model_resolver\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Evaluator] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\n",
      "type_id: 16\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\CsvExampleGen\\\\examples\\\\2\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:65186,xor_checksum:1734928693,sum_checksum:1734928693\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734928897404\n",
      "last_update_time_since_epoch: 1734928897404\n",
      ", artifact_type: id: 16\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'model': [Artifact(artifact: id: 15\n",
      "type_id: 30\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929473656\n",
      "last_update_time_since_epoch: 1734929473656\n",
      ", artifact_type: id: 30\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'baseline_model': []},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 9\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'examples': [Artifact(artifact: id: 1\n",
      "type_id: 16\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\CsvExampleGen\\\\examples\\\\2\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:65186,xor_checksum:1734928693,sum_checksum:1734928693\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734928897404\n",
      "last_update_time_since_epoch: 1734928897404\n",
      ", artifact_type: id: 16\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'model': [Artifact(artifact: id: 15\n",
      "type_id: 30\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929473656\n",
      "last_update_time_since_epoch: 1734929473656\n",
      ", artifact_type: id: 30\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'baseline_model': []}, output_dict=defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Evaluator\\\\evaluation\\\\9\"\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")], 'blessing': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Evaluator\\\\blessing\\\\9\"\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")]}), exec_properties={'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"Attrition\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}'}, execution_output_uri='output\\\\mellisadmyn-pipeline\\\\Evaluator\\\\.system\\\\executor_execution\\\\9\\\\executor_output.pb', stateful_working_dir='output\\\\mellisadmyn-pipeline\\\\Evaluator\\\\.system\\\\stateful_working_dir\\\\20241223-124048.244141', tmp_dir='output\\\\mellisadmyn-pipeline\\\\Evaluator\\\\.system\\\\executor_execution\\\\9\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "    base_type: EVALUATE\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20241223-124048.244141\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Latest_blessed_model_resolver\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Latest_blessed_model_resolver\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"Attrition\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"fairness_indicator_thresholds\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Latest_blessed_model_resolver\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"mellisadmyn-pipeline\"\n",
      ", pipeline_run_id='20241223-124048.244141')\n",
      "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
      "INFO:absl:Copying all content from install dir c:\\Users\\melli\\anaconda3\\envs\\tfx-mlops\\lib\\site-packages\\tfx to temp dir C:\\Users\\melli\\AppData\\Local\\Temp\\tmpya40y0tl\\build\\tfx\n",
      "INFO:absl:Generating a temp setup file at C:\\Users\\melli\\AppData\\Local\\Temp\\tmpya40y0tl\\build\\tfx\\setup.py\n",
      "INFO:absl:Creating temporary sdist package, logs available at C:\\Users\\melli\\AppData\\Local\\Temp\\tmpya40y0tl\\build\\tfx\\setup.log\n",
      "INFO:absl:Added --extra_package=C:\\Users\\melli\\AppData\\Local\\Temp\\tmpya40y0tl\\build\\tfx\\dist\\tfx_ephemeral-1.11.0.tar.gz to beam args\n",
      "INFO:absl:udf_utils.get_fn {'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"Attrition\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}'} 'custom_eval_shared_model'\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"Attrition\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using output\\mellisadmyn-pipeline\\Trainer\\model\\8\\Format-Serving as  model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000234DC2E71F0> and <keras.engine.input_layer.InputLayer object at 0x00000234CE470730>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000234DC2E71F0> and <keras.engine.input_layer.InputLayer object at 0x00000234CE470730>).\n",
      "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
      "INFO:absl:Evaluating model.\n",
      "INFO:absl:udf_utils.get_fn {'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"Attrition\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}'} 'custom_extractors'\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"Attrition\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "}\n",
      "\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"Attrition\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "}\n",
      "\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"Attrition\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000234E950AF10> and <keras.engine.input_layer.InputLayer object at 0x00000234E94FFAF0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000234E950AF10> and <keras.engine.input_layer.InputLayer object at 0x00000234E94FFAF0>).\n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929507   nanos: 858091115 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929507   nanos: 866619110 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929507   nanos: 863603830 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929507   nanos: 871618270 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929507   nanos: 924829244 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929507   nanos: 941357135 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929508   nanos: 103428125 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929508   nanos: 111441850 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929521   nanos: 181748390 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001B1B4BF5F40> and <keras.engine.input_layer.InputLayer object at 0x000001B1B4AF9790>).\" instruction_id: \"bundle_1079\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929521   nanos: 188266515 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000018AA3961640> and <keras.engine.input_layer.InputLayer object at 0x0000018AA384CEB0>).\" instruction_id: \"bundle_1077\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929521   nanos: 187266349 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000250BFACD670> and <keras.engine.input_layer.InputLayer object at 0x00000250BF9BBEE0>).\" instruction_id: \"bundle_1080\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929521   nanos: 207177639 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000024E7C9D0100> and <keras.engine.input_layer.InputLayer object at 0x0000024E7C8BAA00>).\" instruction_id: \"bundle_1078\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929523   nanos: 138968467 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000250C1F75790> and <keras.engine.input_layer.InputLayer object at 0x00000250C1F5A0D0>).\" instruction_id: \"bundle_1080\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929523   nanos: 216479539 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000018AA6E0A670> and <keras.engine.input_layer.InputLayer object at 0x0000018AA6DEAE80>).\" instruction_id: \"bundle_1077\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929523   nanos: 264098405 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001B1B70ACE50> and <keras.engine.input_layer.InputLayer object at 0x000001B1B7093790>).\" instruction_id: \"bundle_1079\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929523   nanos: 276799678 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000024E7EE783D0> and <keras.engine.input_layer.InputLayer object at 0x0000024E7EE57BE0>).\" instruction_id: \"bundle_1078\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929523   nanos: 574901103 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_1080\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929523   nanos: 631059408 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_1077\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929523   nanos: 677348613 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_1079\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929523   nanos: 725500106 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_1078\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929526   nanos: 615671634 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000250C6509FA0> and <keras.engine.input_layer.InputLayer object at 0x00000250C64F9A30>).\" instruction_id: \"bundle_1104\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929526   nanos: 713066577 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000024E043F23D0> and <keras.engine.input_layer.InputLayer object at 0x0000024E03404D90>).\" instruction_id: \"bundle_1102\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929526   nanos: 714126586 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001B1BB645FA0> and <keras.engine.input_layer.InputLayer object at 0x000001B1BB6359A0>).\" instruction_id: \"bundle_1103\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929526   nanos: 734518766 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000018AAB399EE0> and <keras.engine.input_layer.InputLayer object at 0x0000018AAB373FA0>).\" instruction_id: \"bundle_1101\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929528   nanos: 441428184 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000250C89FC9A0> and <keras.engine.input_layer.InputLayer object at 0x00000250C64F4370>).\" instruction_id: \"bundle_1104\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929528   nanos: 602109193 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000024E0590CBE0> and <keras.engine.input_layer.InputLayer object at 0x0000024E0340B160>).\" instruction_id: \"bundle_1102\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929528   nanos: 648749351 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001B1BDB38550> and <keras.engine.input_layer.InputLayer object at 0x000001B1BB637250>).\" instruction_id: \"bundle_1103\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929528   nanos: 668834447 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000018AAD743B80> and <keras.engine.input_layer.InputLayer object at 0x0000018AAB387370>).\" instruction_id: \"bundle_1101\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929530   nanos: 611975908 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001B1BFE67FD0> and <keras.engine.input_layer.InputLayer object at 0x000001B1BFE3CA00>).\" instruction_id: \"bundle_1115\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929530   nanos: 624014139 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000250CAD35520> and <keras.engine.input_layer.InputLayer object at 0x00000250CAD05F40>).\" instruction_id: \"bundle_1116\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929530   nanos: 643041849 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000024E07C44760> and <keras.engine.input_layer.InputLayer object at 0x0000024E07C17100>).\" instruction_id: \"bundle_1114\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929530   nanos: 677291154 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000018AAFBCBFA0> and <keras.engine.input_layer.InputLayer object at 0x0000018AAFB94F10>).\" instruction_id: \"bundle_1113\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929533   nanos: 79792022 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000024E0A13BEE0> and <keras.engine.input_layer.InputLayer object at 0x0000024E0A0E1940>).\" instruction_id: \"bundle_1126\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929533   nanos: 84304332 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000250CD2338E0> and <keras.engine.input_layer.InputLayer object at 0x00000250CD1D0970>).\" instruction_id: \"bundle_1128\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929533   nanos: 179695129 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001B1C2333220> and <keras.engine.input_layer.InputLayer object at 0x000001B1C22E8D90>).\" instruction_id: \"bundle_1127\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929533   nanos: 306042909 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000018AB20E1820> and <keras.engine.input_layer.InputLayer object at 0x0000018AB101D8B0>).\" instruction_id: \"bundle_1125\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929535   nanos: 514555692 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000250CE65CAC0> and <keras.engine.input_layer.InputLayer object at 0x00000250CE5D8A00>).\" instruction_id: \"bundle_1128\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929535   nanos: 528084993 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000024E0B56CC10> and <keras.engine.input_layer.InputLayer object at 0x0000024E0B501670>).\" instruction_id: \"bundle_1126\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929535   nanos: 563155889 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001B1C375B580> and <keras.engine.input_layer.InputLayer object at 0x000001B1C3716F40>).\" instruction_id: \"bundle_1127\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1734929535   nanos: 673485755 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000018AB350DD30> and <keras.engine.input_layer.InputLayer object at 0x0000018AB34A2A00>).\" instruction_id: \"bundle_1125\" log_location: \"c:\\\\Users\\\\melli\\\\anaconda3\\\\envs\\\\tfx-mlops\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "INFO:absl:Evaluation complete. Results written to output\\mellisadmyn-pipeline\\Evaluator\\evaluation\\9.\n",
      "INFO:absl:Checking validation results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\melli\\anaconda3\\envs\\tfx-mlops\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\melli\\anaconda3\\envs\\tfx-mlops\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:absl:Blessing result True written to output\\mellisadmyn-pipeline\\Evaluator\\blessing\\9.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 9 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Evaluator\\\\evaluation\\\\9\"\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")], 'blessing': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Evaluator\\\\blessing\\\\9\"\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")]}) for execution 9\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:node Evaluator is finished.\n",
      "INFO:absl:node Pusher is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20241223-124048.244141\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"output\\\\\\\\serving_model\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 15\n",
      "type_id: 30\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929473656\n",
      "last_update_time_since_epoch: 1734929473656\n",
      ", artifact_type: id: 30\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_blessing': [Artifact(artifact: id: 18\n",
      "type_id: 34\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Evaluator\\\\blessing\\\\9\"\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"output\\\\mellisadmyn-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 15\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929542587\n",
      "last_update_time_since_epoch: 1734929542587\n",
      ", artifact_type: id: 34\n",
      "name: \"ModelBlessing\"\n",
      ")]},)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 10\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=10, input_dict={'model': [Artifact(artifact: id: 15\n",
      "type_id: 30\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929473656\n",
      "last_update_time_since_epoch: 1734929473656\n",
      ", artifact_type: id: 30\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_blessing': [Artifact(artifact: id: 18\n",
      "type_id: 34\n",
      "uri: \"output\\\\mellisadmyn-pipeline\\\\Evaluator\\\\blessing\\\\9\"\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"output\\\\mellisadmyn-pipeline\\\\Trainer\\\\model\\\\8\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 15\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1734929542587\n",
      "last_update_time_since_epoch: 1734929542587\n",
      ", artifact_type: id: 34\n",
      "name: \"ModelBlessing\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Pusher\\\\pushed_model\\\\10\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"output\\\\\\\\serving_model\"\\n  }\\n}', 'custom_config': 'null'}, execution_output_uri='output\\\\mellisadmyn-pipeline\\\\Pusher\\\\.system\\\\executor_execution\\\\10\\\\executor_output.pb', stateful_working_dir='output\\\\mellisadmyn-pipeline\\\\Pusher\\\\.system\\\\stateful_working_dir\\\\20241223-124048.244141', tmp_dir='output\\\\mellisadmyn-pipeline\\\\Pusher\\\\.system\\\\executor_execution\\\\10\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20241223-124048.244141\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"mellisadmyn-pipeline.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20241223-124048.244141\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"mellisadmyn-pipeline.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"output\\\\\\\\serving_model\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"mellisadmyn-pipeline\"\n",
      ", pipeline_run_id='20241223-124048.244141')\n",
      "INFO:absl:Model version: 1734929542\n",
      "INFO:absl:Model written to serving path output\\serving_model\\1734929542.\n",
      "INFO:absl:Model pushed to output\\mellisadmyn-pipeline\\Pusher\\pushed_model\\10.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 10 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"output\\\\mellisadmyn-pipeline\\\\Pusher\\\\pushed_model\\\\10\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 10\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:node Pusher is finished.\n"
     ]
    }
   ],
   "source": [
    "from modules.components import init_components\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "# Konfigurasi pipeline\n",
    "config = {\n",
    "    \"DATA_ROOT\": DATA_ROOT,\n",
    "    \"training_module\": TRAINER_MODULE_FILE,\n",
    "    \"transform_module\": TRANSFORM_MODULE_FILE,\n",
    "    \"tuner_module\": TUNER_MODULE_FILE,\n",
    "    \"training_steps\": 1000,\n",
    "    \"eval_steps\": 250,\n",
    "    \"serving_model_dir\": serving_model_dir,\n",
    "}\n",
    "\n",
    "# Inisialisasi komponen pipeline\n",
    "components = init_components(config)\n",
    "\n",
    "# Membuat pipeline\n",
    "pipeline = init_local_pipeline(components, pipeline_root)\n",
    "\n",
    "# Menjalankan pipeline\n",
    "BeamDagRunner().run(pipeline=pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205637e6b30841adb601858f4ebc701e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'Overall', 'metrics':…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "# Path ke folder evaluation\n",
    "eval_result = tfma.load_eval_result(output_path='output/mellisadmyn-pipeline/Evaluator/evaluation/9')\n",
    "\n",
    "# Menampilkan metrik dalam bentuk tabel\n",
    "tfma.view.render_slicing_metrics(eval_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_specs {\n",
      "  label_key: \"Attrition\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.8\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "}\n",
      "\n",
      "[((), {'': {'': {'binary_accuracy': {'doubleValue': 0.8363636363636363}, 'loss': {'doubleValue': 0.4381735324859619}, 'example_count': {'doubleValue': 220.0}, 'auc': {'doubleValue': 0.716339760638298}, 'precision': {'doubleValue': 0.4230769230769231}, 'recall': {'doubleValue': 0.34375}}}})]\n"
     ]
    }
   ],
   "source": [
    "eval_result = tfma.load_eval_result(output_path='output/mellisadmyn-pipeline/Evaluator/evaluation/9')\n",
    "\n",
    "# Debugging: Cek isi dari eval_result\n",
    "print(eval_result.config)\n",
    "print(eval_result.slicing_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Evaluasi Model:\n",
      "==================================================\n",
      "Slice: ()\n",
      "  : {'binary_accuracy': {'doubleValue': 0.8363636363636363}, 'loss': {'doubleValue': 0.4381735324859619}, 'example_count': {'doubleValue': 220.0}, 'auc': {'doubleValue': 0.716339760638298}, 'precision': {'doubleValue': 0.4230769230769231}, 'recall': {'doubleValue': 0.34375}}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "# Path ke hasil evaluasi TFMA\n",
    "evaluation_path = 'output/mellisadmyn-pipeline/Evaluator/evaluation/9'  # Ganti sesuai path Anda\n",
    "\n",
    "# Load hasil evaluasi\n",
    "eval_result = tfma.load_eval_result(output_path=evaluation_path)\n",
    "\n",
    "# Print slicing metrics dengan format yang mudah dibaca\n",
    "print(\"Hasil Evaluasi Model:\")\n",
    "print(\"=\" * 50)\n",
    "for slicing_metric in eval_result.slicing_metrics:\n",
    "    slice_key, metrics = slicing_metric\n",
    "    print(f\"Slice: {slice_key}\")  # Key untuk slicing (misal: semua data atau slice tertentu)\n",
    "    for metric_name, metric_value in metrics[''].items():  # Iterasi setiap metrik\n",
    "        print(f\"  {metric_name}: {metric_value}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfx-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
